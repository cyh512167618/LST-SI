Sat 03 Sep 2022 11:27:37 INFO  
General Hyper Parameters:
gpu_id = 0
use_gpu = True
seed = 212
state = INFO
reproducibility = True
data_path = dataset/amazon-beauty
show_progress = True
save_dataset = False
save_dataloaders = False
benchmark_filename = None

Training Hyper Parameters:
checkpoint_dir = saved
epochs = 50
train_batch_size = 128
learner = adam
learning_rate = 0.0001
eval_step = 2
stopping_step = 10
clip_grad_norm = None
weight_decay = 0.0
loss_decimal_place = 4

Evaluation Hyper Parameters:
eval_args = {'split': {'LS': 'valid_and_test'}, 'group_by': 'user', 'order': 'TO', 'mode': 'full'}
metrics = ['Recall', 'NDCG']
topk = [3, 5, 10, 20]
valid_metric = Recall@20
valid_metric_bigger = True
eval_batch_size = 128
metric_decimal_place = 4

Dataset Hyper Parameters:
field_separator = 	
seq_separator =  
USER_ID_FIELD = user_id
ITEM_ID_FIELD = item_id
RATING_FIELD = rating
TIME_FIELD = timestamp
seq_len = None
LABEL_FIELD = label
threshold = None
NEG_PREFIX = neg_
load_col = {'inter': ['user_id', 'item_id', 'rating', 'timestamp'], 'item': ['item_id', 'title', 'sales_rank', 'price', 'brand', 'categories', 'sales_type']}
unload_col = None
unused_col = None
additional_feat_suffix = None
rm_dup_inter = None
val_interval = None
filter_inter_by_user_or_item = True
user_inter_num_interval = [5,inf)
item_inter_num_interval = [5,inf)
alias_of_user_id = None
alias_of_item_id = None
alias_of_entity_id = None
alias_of_relation_id = None
preload_weight = None
normalize_field = None
normalize_all = None
ITEM_LIST_LENGTH_FIELD = item_length
LIST_SUFFIX = _list
MAX_ITEM_LIST_LENGTH = 50
POSITION_FIELD = position_id
HEAD_ENTITY_ID_FIELD = head_id
TAIL_ENTITY_ID_FIELD = tail_id
RELATION_ID_FIELD = relation_id
ENTITY_ID_FIELD = entity_id

Other Hyper Parameters: 
neg_sampling = None
multi_gpus = False
repeatable = True
embedding_size = 64
dropout_prob = 0.4
reg_weight = 0.0001
nv = 8
nh = 16
loss_type = BPR
MODEL_TYPE = ModelType.SEQUENTIAL
n_layers = 4
n_heads = 8
hidden_size = 256
attribute_hidden_size = [64]
inner_size = 256
hidden_dropout_prob = 0.5
attn_dropout_prob = 0.3
hidden_act = gelu
layer_norm_eps = 1e-12
initializer_range = 0.02
selected_features = ['categories']
pooling_mode = sum
fusion_type = gate
lamdas = [10]
attribute_predictor = linear
MODEL_INPUT_TYPE = InputType.PAIRWISE
eval_type = EvaluatorType.RANKING
device = cuda
train_neg_sample_args = {'strategy': 'none'}
eval_neg_sample_args = {'strategy': 'full', 'distribution': 'uniform'}


Sat 03 Sep 2022 11:27:48 INFO  amazon-beauty
The number of users: 22364
Average actions of users: 8.876358270357287
The number of items: 12102
Average actions of items: 16.403768283612923
The number of inters: 198502
The sparsity of the dataset: 99.92665707018277%
Remain Fields: ['user_id', 'item_id', 'rating', 'timestamp', 'title', 'sales_type', 'sales_rank', 'categories', 'price', 'brand']
Sat 03 Sep 2022 11:27:52 INFO  [Training]: train_batch_size = [128] negative sampling: [None]
Sat 03 Sep 2022 11:27:52 INFO  [Evaluation]: eval_batch_size = [128] eval_args: [{'split': {'LS': 'valid_and_test'}, 'group_by': 'user', 'order': 'TO', 'mode': 'full'}]
Sat 03 Sep 2022 11:27:54 INFO  Caser(
  (user_embedding): Embedding(22364, 64, padding_idx=0)
  (item_embedding): Embedding(12102, 64, padding_idx=0)
  (conv_v): Conv2d(1, 8, kernel_size=(50, 1), stride=(1, 1))
  (conv_h): ModuleList(
    (0): Conv2d(1, 16, kernel_size=(1, 64), stride=(1, 1))
    (1): Conv2d(1, 16, kernel_size=(2, 64), stride=(1, 1))
    (2): Conv2d(1, 16, kernel_size=(3, 64), stride=(1, 1))
    (3): Conv2d(1, 16, kernel_size=(4, 64), stride=(1, 1))
    (4): Conv2d(1, 16, kernel_size=(5, 64), stride=(1, 1))
    (5): Conv2d(1, 16, kernel_size=(6, 64), stride=(1, 1))
    (6): Conv2d(1, 16, kernel_size=(7, 64), stride=(1, 1))
    (7): Conv2d(1, 16, kernel_size=(8, 64), stride=(1, 1))
    (8): Conv2d(1, 16, kernel_size=(9, 64), stride=(1, 1))
    (9): Conv2d(1, 16, kernel_size=(10, 64), stride=(1, 1))
    (10): Conv2d(1, 16, kernel_size=(11, 64), stride=(1, 1))
    (11): Conv2d(1, 16, kernel_size=(12, 64), stride=(1, 1))
    (12): Conv2d(1, 16, kernel_size=(13, 64), stride=(1, 1))
    (13): Conv2d(1, 16, kernel_size=(14, 64), stride=(1, 1))
    (14): Conv2d(1, 16, kernel_size=(15, 64), stride=(1, 1))
    (15): Conv2d(1, 16, kernel_size=(16, 64), stride=(1, 1))
    (16): Conv2d(1, 16, kernel_size=(17, 64), stride=(1, 1))
    (17): Conv2d(1, 16, kernel_size=(18, 64), stride=(1, 1))
    (18): Conv2d(1, 16, kernel_size=(19, 64), stride=(1, 1))
    (19): Conv2d(1, 16, kernel_size=(20, 64), stride=(1, 1))
    (20): Conv2d(1, 16, kernel_size=(21, 64), stride=(1, 1))
    (21): Conv2d(1, 16, kernel_size=(22, 64), stride=(1, 1))
    (22): Conv2d(1, 16, kernel_size=(23, 64), stride=(1, 1))
    (23): Conv2d(1, 16, kernel_size=(24, 64), stride=(1, 1))
    (24): Conv2d(1, 16, kernel_size=(25, 64), stride=(1, 1))
    (25): Conv2d(1, 16, kernel_size=(26, 64), stride=(1, 1))
    (26): Conv2d(1, 16, kernel_size=(27, 64), stride=(1, 1))
    (27): Conv2d(1, 16, kernel_size=(28, 64), stride=(1, 1))
    (28): Conv2d(1, 16, kernel_size=(29, 64), stride=(1, 1))
    (29): Conv2d(1, 16, kernel_size=(30, 64), stride=(1, 1))
    (30): Conv2d(1, 16, kernel_size=(31, 64), stride=(1, 1))
    (31): Conv2d(1, 16, kernel_size=(32, 64), stride=(1, 1))
    (32): Conv2d(1, 16, kernel_size=(33, 64), stride=(1, 1))
    (33): Conv2d(1, 16, kernel_size=(34, 64), stride=(1, 1))
    (34): Conv2d(1, 16, kernel_size=(35, 64), stride=(1, 1))
    (35): Conv2d(1, 16, kernel_size=(36, 64), stride=(1, 1))
    (36): Conv2d(1, 16, kernel_size=(37, 64), stride=(1, 1))
    (37): Conv2d(1, 16, kernel_size=(38, 64), stride=(1, 1))
    (38): Conv2d(1, 16, kernel_size=(39, 64), stride=(1, 1))
    (39): Conv2d(1, 16, kernel_size=(40, 64), stride=(1, 1))
    (40): Conv2d(1, 16, kernel_size=(41, 64), stride=(1, 1))
    (41): Conv2d(1, 16, kernel_size=(42, 64), stride=(1, 1))
    (42): Conv2d(1, 16, kernel_size=(43, 64), stride=(1, 1))
    (43): Conv2d(1, 16, kernel_size=(44, 64), stride=(1, 1))
    (44): Conv2d(1, 16, kernel_size=(45, 64), stride=(1, 1))
    (45): Conv2d(1, 16, kernel_size=(46, 64), stride=(1, 1))
    (46): Conv2d(1, 16, kernel_size=(47, 64), stride=(1, 1))
    (47): Conv2d(1, 16, kernel_size=(48, 64), stride=(1, 1))
    (48): Conv2d(1, 16, kernel_size=(49, 64), stride=(1, 1))
    (49): Conv2d(1, 16, kernel_size=(50, 64), stride=(1, 1))
  )
  (fc1): Linear(in_features=1312, out_features=64, bias=True)
  (fc2): Linear(in_features=128, out_features=64, bias=True)
  (dropout): Dropout(p=0.4, inplace=False)
  (ac_conv): ReLU()
  (ac_fc): ReLU()
  (reg_loss): RegLoss()
  (loss_fct): BPRLoss()
)
Trainable parameters: 3604920
