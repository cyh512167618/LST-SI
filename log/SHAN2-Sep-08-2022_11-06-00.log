Thu 08 Sep 2022 11:06:00 INFO  
General Hyper Parameters:
gpu_id = 0
use_gpu = True
seed = 212
state = INFO
reproducibility = True
data_path = D:\papers\Decoupled Side Information Fusion for Sequential Recommendatoin\DIF-SR-main\DIF-SR-main\recbole\config\../dataset_example/ml-100k
show_progress = True
save_dataset = False
save_dataloaders = False
benchmark_filename = None

Training Hyper Parameters:
checkpoint_dir = saved
epochs = 50
train_batch_size = 256
learner = adam
learning_rate = 0.0001
eval_step = 2
stopping_step = 10
clip_grad_norm = None
weight_decay = 0.0
loss_decimal_place = 4

Evaluation Hyper Parameters:
eval_args = {'split': {'LS': 'valid_and_test'}, 'group_by': 'user', 'order': 'TO', 'mode': 'full'}
metrics = ['Recall', 'NDCG']
topk = [3, 5, 10, 20]
valid_metric = Recall@20
valid_metric_bigger = True
eval_batch_size = 256
metric_decimal_place = 4

Dataset Hyper Parameters:
field_separator = 	
seq_separator =  
USER_ID_FIELD = user_id
ITEM_ID_FIELD = item_id
RATING_FIELD = rating
TIME_FIELD = timestamp
seq_len = None
LABEL_FIELD = label
threshold = None
NEG_PREFIX = neg_
load_col = {'inter': ['user_id', 'item_id', 'rating', 'timestamp'], 'item': ['item_id', 'movie_title', 'release_year', 'class']}
unload_col = None
unused_col = None
additional_feat_suffix = None
rm_dup_inter = first
val_interval = None
filter_inter_by_user_or_item = True
user_inter_num_interval = [5,inf)
item_inter_num_interval = [5,inf)
alias_of_user_id = None
alias_of_item_id = None
alias_of_entity_id = None
alias_of_relation_id = None
preload_weight = None
normalize_field = None
normalize_all = True
ITEM_LIST_LENGTH_FIELD = item_length
LIST_SUFFIX = _list
MAX_ITEM_LIST_LENGTH = 50
POSITION_FIELD = position_id
HEAD_ENTITY_ID_FIELD = head_id
TAIL_ENTITY_ID_FIELD = tail_id
RELATION_ID_FIELD = relation_id
ENTITY_ID_FIELD = entity_id

Other Hyper Parameters: 
neg_sampling = None
multi_gpus = False
repeatable = True
embedding_size = 256
short_item_length = 2
n_layers = 4
n_heads = 8
attribute_hidden_size = [64]
inner_size = 256
hidden_dropout_prob = 0.5
attn_dropout_prob = 0.3
hidden_act = gelu
layer_norm_eps = 1e-12
initializer_range = 0.02
selected_features = ['class']
pooling_mode = sum
loss_type = CE
weight_sharing = not
fusion_type = gate
lamdas = [10]
attribute_predictor = linear
step = 1
reg_weight = [0.01, 0.0001]
MODEL_TYPE = ModelType.SEQUENTIAL
hidden_size = 256
MODEL_INPUT_TYPE = InputType.POINTWISE
eval_type = EvaluatorType.RANKING
device = cuda
train_neg_sample_args = {'strategy': 'none'}
eval_neg_sample_args = {'strategy': 'full', 'distribution': 'uniform'}


Thu 08 Sep 2022 11:06:00 INFO  Records in original dataset have been sorted by value of [timestamp] in ascending order.
Thu 08 Sep 2022 11:06:00 INFO  ml-100k
The number of users: 944
Average actions of users: 105.28844114528101
The number of items: 1350
Average actions of items: 73.6004447739066
The number of inters: 99287
The sparsity of the dataset: 92.20911801632141%
Remain Fields: ['user_id', 'item_id', 'rating', 'timestamp', 'movie_title', 'release_year', 'class']
Thu 08 Sep 2022 11:06:02 INFO  [Training]: train_batch_size = [256] negative sampling: [None]
Thu 08 Sep 2022 11:06:02 INFO  [Evaluation]: eval_batch_size = [256] eval_args: [{'split': {'LS': 'valid_and_test'}, 'group_by': 'user', 'order': 'TO', 'mode': 'full'}]
Thu 08 Sep 2022 11:06:03 INFO  SHAN2(
  (feature_embed_layer_list): ModuleList(
    (0): FeatureSeqEmbLayer()
  )
  (trm_encoder): DIFTransformerEncoder(
    (layer): ModuleList(
      (0): DIFTransformerLayer(
        (multi_head_attention): DIFMultiHeadAttention(
          (query): Linear(in_features=256, out_features=256, bias=True)
          (key): Linear(in_features=256, out_features=256, bias=True)
          (value): Linear(in_features=256, out_features=256, bias=True)
          (query_p): Linear(in_features=256, out_features=256, bias=True)
          (key_p): Linear(in_features=256, out_features=256, bias=True)
          (query_layers): ModuleList(
            (0): Linear(in_features=64, out_features=64, bias=True)
          )
          (key_layers): ModuleList(
            (0): Linear(in_features=64, out_features=64, bias=True)
          )
          (fusion_layer): VanillaAttention(
            (projection): Sequential(
              (0): Linear(in_features=50, out_features=50, bias=True)
              (1): ReLU(inplace=True)
              (2): Linear(in_features=50, out_features=1, bias=True)
            )
          )
          (attn_dropout): Dropout(p=0.3, inplace=False)
          (dense): Linear(in_features=256, out_features=256, bias=True)
          (LayerNorm): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
          (out_dropout): Dropout(p=0.5, inplace=False)
        )
        (feed_forward): FeedForward(
          (dense_1): Linear(in_features=256, out_features=256, bias=True)
          (dense_2): Linear(in_features=256, out_features=256, bias=True)
          (LayerNorm): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
          (dropout): Dropout(p=0.5, inplace=False)
        )
      )
      (1): DIFTransformerLayer(
        (multi_head_attention): DIFMultiHeadAttention(
          (query): Linear(in_features=256, out_features=256, bias=True)
          (key): Linear(in_features=256, out_features=256, bias=True)
          (value): Linear(in_features=256, out_features=256, bias=True)
          (query_p): Linear(in_features=256, out_features=256, bias=True)
          (key_p): Linear(in_features=256, out_features=256, bias=True)
          (query_layers): ModuleList(
            (0): Linear(in_features=64, out_features=64, bias=True)
          )
          (key_layers): ModuleList(
            (0): Linear(in_features=64, out_features=64, bias=True)
          )
          (fusion_layer): VanillaAttention(
            (projection): Sequential(
              (0): Linear(in_features=50, out_features=50, bias=True)
              (1): ReLU(inplace=True)
              (2): Linear(in_features=50, out_features=1, bias=True)
            )
          )
          (attn_dropout): Dropout(p=0.3, inplace=False)
          (dense): Linear(in_features=256, out_features=256, bias=True)
          (LayerNorm): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
          (out_dropout): Dropout(p=0.5, inplace=False)
        )
        (feed_forward): FeedForward(
          (dense_1): Linear(in_features=256, out_features=256, bias=True)
          (dense_2): Linear(in_features=256, out_features=256, bias=True)
          (LayerNorm): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
          (dropout): Dropout(p=0.5, inplace=False)
        )
      )
      (2): DIFTransformerLayer(
        (multi_head_attention): DIFMultiHeadAttention(
          (query): Linear(in_features=256, out_features=256, bias=True)
          (key): Linear(in_features=256, out_features=256, bias=True)
          (value): Linear(in_features=256, out_features=256, bias=True)
          (query_p): Linear(in_features=256, out_features=256, bias=True)
          (key_p): Linear(in_features=256, out_features=256, bias=True)
          (query_layers): ModuleList(
            (0): Linear(in_features=64, out_features=64, bias=True)
          )
          (key_layers): ModuleList(
            (0): Linear(in_features=64, out_features=64, bias=True)
          )
          (fusion_layer): VanillaAttention(
            (projection): Sequential(
              (0): Linear(in_features=50, out_features=50, bias=True)
              (1): ReLU(inplace=True)
              (2): Linear(in_features=50, out_features=1, bias=True)
            )
          )
          (attn_dropout): Dropout(p=0.3, inplace=False)
          (dense): Linear(in_features=256, out_features=256, bias=True)
          (LayerNorm): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
          (out_dropout): Dropout(p=0.5, inplace=False)
        )
        (feed_forward): FeedForward(
          (dense_1): Linear(in_features=256, out_features=256, bias=True)
          (dense_2): Linear(in_features=256, out_features=256, bias=True)
          (LayerNorm): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
          (dropout): Dropout(p=0.5, inplace=False)
        )
      )
      (3): DIFTransformerLayer(
        (multi_head_attention): DIFMultiHeadAttention(
          (query): Linear(in_features=256, out_features=256, bias=True)
          (key): Linear(in_features=256, out_features=256, bias=True)
          (value): Linear(in_features=256, out_features=256, bias=True)
          (query_p): Linear(in_features=256, out_features=256, bias=True)
          (key_p): Linear(in_features=256, out_features=256, bias=True)
          (query_layers): ModuleList(
            (0): Linear(in_features=64, out_features=64, bias=True)
          )
          (key_layers): ModuleList(
            (0): Linear(in_features=64, out_features=64, bias=True)
          )
          (fusion_layer): VanillaAttention(
            (projection): Sequential(
              (0): Linear(in_features=50, out_features=50, bias=True)
              (1): ReLU(inplace=True)
              (2): Linear(in_features=50, out_features=1, bias=True)
            )
          )
          (attn_dropout): Dropout(p=0.3, inplace=False)
          (dense): Linear(in_features=256, out_features=256, bias=True)
          (LayerNorm): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
          (out_dropout): Dropout(p=0.5, inplace=False)
        )
        (feed_forward): FeedForward(
          (dense_1): Linear(in_features=256, out_features=256, bias=True)
          (dense_2): Linear(in_features=256, out_features=256, bias=True)
          (LayerNorm): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
          (dropout): Dropout(p=0.5, inplace=False)
        )
      )
    )
  )
  (ap): ModuleList(
    (0): Linear(in_features=256, out_features=20, bias=True)
  )
  (item_embedding): Embedding(1350, 256, padding_idx=0)
  (user_embedding): Embedding(944, 256)
  (position_embedding): Embedding(50, 256)
  (user_embedding_linear): Linear(in_features=256, out_features=256, bias=False)
  (item_embedding_linear): Linear(in_features=256, out_features=256, bias=False)
  (sigmoid): Sigmoid()
  (long_w): Linear(in_features=256, out_features=256, bias=False)
  (long_short_w): Linear(in_features=256, out_features=256, bias=True)
  (relu): ReLU()
  (LayerNorm): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
  (dropout): Dropout(p=0.5, inplace=False)
  (loss_fct): CrossEntropyLoss()
  (attribute_loss_fct): BCEWithLogitsLoss()
)
Trainable parameters: 3021240
Thu 08 Sep 2022 11:07:12 INFO  epoch 0 training [time: 68.93s, train loss: 3884.1562]
Thu 08 Sep 2022 11:08:20 INFO  epoch 1 training [time: 68.04s, train loss: 3655.7193]
Thu 08 Sep 2022 11:08:21 INFO  epoch 1 evaluating [time: 0.21s, valid_score: 0.115600]
Thu 08 Sep 2022 11:08:21 INFO  valid result: 
recall@3 : 0.0265    recall@5 : 0.0424    recall@10 : 0.0657    recall@20 : 0.1156    ndcg@3 : 0.0172    ndcg@5 : 0.0236    ndcg@10 : 0.0311    ndcg@20 : 0.0436    
Thu 08 Sep 2022 11:08:21 INFO  Saving current best: saved\SHAN2-Sep-08-2022_11-06-04.pth
Thu 08 Sep 2022 11:09:30 INFO  epoch 2 training [time: 68.86s, train loss: 3508.3106]
Thu 08 Sep 2022 11:10:39 INFO  epoch 3 training [time: 68.80s, train loss: 3393.9925]
Thu 08 Sep 2022 11:10:39 INFO  epoch 3 evaluating [time: 0.21s, valid_score: 0.194100]
Thu 08 Sep 2022 11:10:39 INFO  valid result: 
recall@3 : 0.0329    recall@5 : 0.0604    recall@10 : 0.1113    recall@20 : 0.1941    ndcg@3 : 0.0245    ndcg@5 : 0.0359    ndcg@10 : 0.052    ndcg@20 : 0.073    
Thu 08 Sep 2022 11:10:39 INFO  Saving current best: saved\SHAN2-Sep-08-2022_11-06-04.pth
Thu 08 Sep 2022 11:11:48 INFO  epoch 4 training [time: 68.46s, train loss: 3329.3126]
Thu 08 Sep 2022 11:12:56 INFO  epoch 5 training [time: 68.59s, train loss: 3294.3535]
Thu 08 Sep 2022 11:12:57 INFO  epoch 5 evaluating [time: 0.21s, valid_score: 0.199400]
Thu 08 Sep 2022 11:12:57 INFO  valid result: 
recall@3 : 0.0382    recall@5 : 0.0647    recall@10 : 0.123    recall@20 : 0.1994    ndcg@3 : 0.0264    ndcg@5 : 0.0374    ndcg@10 : 0.0557    ndcg@20 : 0.0746    
Thu 08 Sep 2022 11:12:57 INFO  Saving current best: saved\SHAN2-Sep-08-2022_11-06-04.pth
Thu 08 Sep 2022 11:14:05 INFO  epoch 6 training [time: 68.42s, train loss: 3269.8262]
Thu 08 Sep 2022 11:15:14 INFO  epoch 7 training [time: 68.51s, train loss: 3249.2365]
Thu 08 Sep 2022 11:15:14 INFO  epoch 7 evaluating [time: 0.21s, valid_score: 0.203600]
Thu 08 Sep 2022 11:15:14 INFO  valid result: 
recall@3 : 0.0361    recall@5 : 0.0647    recall@10 : 0.1283    recall@20 : 0.2036    ndcg@3 : 0.0279    ndcg@5 : 0.0398    ndcg@10 : 0.06    ndcg@20 : 0.0791    
Thu 08 Sep 2022 11:15:14 INFO  Saving current best: saved\SHAN2-Sep-08-2022_11-06-04.pth
Thu 08 Sep 2022 11:16:23 INFO  epoch 8 training [time: 68.27s, train loss: 3233.9692]
Thu 08 Sep 2022 11:17:30 INFO  epoch 9 training [time: 67.65s, train loss: 3219.0725]
Thu 08 Sep 2022 11:17:31 INFO  epoch 9 evaluating [time: 0.21s, valid_score: 0.200400]
Thu 08 Sep 2022 11:17:31 INFO  valid result: 
recall@3 : 0.0392    recall@5 : 0.0615    recall@10 : 0.1135    recall@20 : 0.2004    ndcg@3 : 0.0285    ndcg@5 : 0.0379    ndcg@10 : 0.0544    ndcg@20 : 0.0765    
Thu 08 Sep 2022 11:18:38 INFO  epoch 10 training [time: 67.60s, train loss: 3207.8662]
Thu 08 Sep 2022 11:19:46 INFO  epoch 11 training [time: 67.55s, train loss: 3198.1302]
Thu 08 Sep 2022 11:19:46 INFO  epoch 11 evaluating [time: 0.21s, valid_score: 0.214200]
Thu 08 Sep 2022 11:19:46 INFO  valid result: 
recall@3 : 0.0339    recall@5 : 0.0668    recall@10 : 0.1135    recall@20 : 0.2142    ndcg@3 : 0.0242    ndcg@5 : 0.0376    ndcg@10 : 0.0525    ndcg@20 : 0.0778    
Thu 08 Sep 2022 11:19:46 INFO  Saving current best: saved\SHAN2-Sep-08-2022_11-06-04.pth
Thu 08 Sep 2022 11:20:54 INFO  epoch 12 training [time: 67.55s, train loss: 3188.3499]
Thu 08 Sep 2022 11:22:02 INFO  epoch 13 training [time: 67.70s, train loss: 3178.4555]
Thu 08 Sep 2022 11:22:02 INFO  epoch 13 evaluating [time: 0.21s, valid_score: 0.220600]
Thu 08 Sep 2022 11:22:02 INFO  valid result: 
recall@3 : 0.0445    recall@5 : 0.0594    recall@10 : 0.1177    recall@20 : 0.2206    ndcg@3 : 0.0326    ndcg@5 : 0.0387    ndcg@10 : 0.057    ndcg@20 : 0.0827    
Thu 08 Sep 2022 11:22:02 INFO  Saving current best: saved\SHAN2-Sep-08-2022_11-06-04.pth
Thu 08 Sep 2022 11:23:10 INFO  epoch 14 training [time: 67.67s, train loss: 3170.2954]
Thu 08 Sep 2022 11:24:17 INFO  epoch 15 training [time: 67.54s, train loss: 3162.8403]
Thu 08 Sep 2022 11:24:18 INFO  epoch 15 evaluating [time: 0.21s, valid_score: 0.213100]
Thu 08 Sep 2022 11:24:18 INFO  valid result: 
recall@3 : 0.0382    recall@5 : 0.0657    recall@10 : 0.1251    recall@20 : 0.2131    ndcg@3 : 0.0278    ndcg@5 : 0.0391    ndcg@10 : 0.0583    ndcg@20 : 0.0804    
Thu 08 Sep 2022 11:25:25 INFO  epoch 16 training [time: 67.52s, train loss: 3154.8169]
Thu 08 Sep 2022 11:26:33 INFO  epoch 17 training [time: 67.59s, train loss: 3148.5276]
Thu 08 Sep 2022 11:26:33 INFO  epoch 17 evaluating [time: 0.21s, valid_score: 0.217400]
Thu 08 Sep 2022 11:26:33 INFO  valid result: 
recall@3 : 0.0382    recall@5 : 0.0679    recall@10 : 0.1209    recall@20 : 0.2174    ndcg@3 : 0.0274    ndcg@5 : 0.0396    ndcg@10 : 0.0567    ndcg@20 : 0.0809    
Thu 08 Sep 2022 11:27:40 INFO  epoch 18 training [time: 67.64s, train loss: 3139.6396]
Thu 08 Sep 2022 11:28:48 INFO  epoch 19 training [time: 67.80s, train loss: 3133.0154]
Thu 08 Sep 2022 11:28:48 INFO  epoch 19 evaluating [time: 0.21s, valid_score: 0.226900]
Thu 08 Sep 2022 11:28:48 INFO  valid result: 
recall@3 : 0.053    recall@5 : 0.0764    recall@10 : 0.1209    recall@20 : 0.2269    ndcg@3 : 0.0364    ndcg@5 : 0.046    ndcg@10 : 0.0601    ndcg@20 : 0.0867    
Thu 08 Sep 2022 11:28:49 INFO  Saving current best: saved\SHAN2-Sep-08-2022_11-06-04.pth
Thu 08 Sep 2022 11:29:57 INFO  epoch 20 training [time: 67.68s, train loss: 3126.0751]
Thu 08 Sep 2022 11:31:04 INFO  epoch 21 training [time: 67.67s, train loss: 3118.8499]
Thu 08 Sep 2022 11:31:04 INFO  epoch 21 evaluating [time: 0.21s, valid_score: 0.219500]
Thu 08 Sep 2022 11:31:04 INFO  valid result: 
recall@3 : 0.0414    recall@5 : 0.0774    recall@10 : 0.1304    recall@20 : 0.2195    ndcg@3 : 0.0294    ndcg@5 : 0.0442    ndcg@10 : 0.0613    ndcg@20 : 0.0838    
Thu 08 Sep 2022 11:32:12 INFO  epoch 22 training [time: 67.67s, train loss: 3112.5853]
Thu 08 Sep 2022 11:33:20 INFO  epoch 23 training [time: 67.66s, train loss: 3105.6520]
Thu 08 Sep 2022 11:33:20 INFO  epoch 23 evaluating [time: 0.21s, valid_score: 0.229100]
Thu 08 Sep 2022 11:33:20 INFO  valid result: 
recall@3 : 0.0477    recall@5 : 0.0732    recall@10 : 0.123    recall@20 : 0.2291    ndcg@3 : 0.0344    ndcg@5 : 0.0448    ndcg@10 : 0.0608    ndcg@20 : 0.0874    
Thu 08 Sep 2022 11:33:20 INFO  Saving current best: saved\SHAN2-Sep-08-2022_11-06-04.pth
Thu 08 Sep 2022 11:34:28 INFO  epoch 24 training [time: 67.67s, train loss: 3097.2703]
Thu 08 Sep 2022 11:35:36 INFO  epoch 25 training [time: 67.78s, train loss: 3091.5931]
Thu 08 Sep 2022 11:35:36 INFO  epoch 25 evaluating [time: 0.21s, valid_score: 0.222700]
Thu 08 Sep 2022 11:35:36 INFO  valid result: 
recall@3 : 0.0498    recall@5 : 0.0721    recall@10 : 0.1379    recall@20 : 0.2227    ndcg@3 : 0.0343    ndcg@5 : 0.0434    ndcg@10 : 0.0645    ndcg@20 : 0.0855    
Thu 08 Sep 2022 11:36:44 INFO  epoch 26 training [time: 67.67s, train loss: 3084.8022]
Thu 08 Sep 2022 11:37:51 INFO  epoch 27 training [time: 67.69s, train loss: 3078.2598]
Thu 08 Sep 2022 11:37:52 INFO  epoch 27 evaluating [time: 0.21s, valid_score: 0.223800]
Thu 08 Sep 2022 11:37:52 INFO  valid result: 
recall@3 : 0.0445    recall@5 : 0.0647    recall@10 : 0.123    recall@20 : 0.2238    ndcg@3 : 0.034    ndcg@5 : 0.0423    ndcg@10 : 0.061    ndcg@20 : 0.0862    
Thu 08 Sep 2022 11:38:59 INFO  epoch 28 training [time: 67.68s, train loss: 3071.4992]
Thu 08 Sep 2022 11:40:07 INFO  epoch 29 training [time: 67.79s, train loss: 3064.9904]
Thu 08 Sep 2022 11:40:07 INFO  epoch 29 evaluating [time: 0.21s, valid_score: 0.221600]
Thu 08 Sep 2022 11:40:07 INFO  valid result: 
recall@3 : 0.0445    recall@5 : 0.0668    recall@10 : 0.1273    recall@20 : 0.2216    ndcg@3 : 0.0328    ndcg@5 : 0.0421    ndcg@10 : 0.0614    ndcg@20 : 0.0851    
Thu 08 Sep 2022 11:41:15 INFO  epoch 30 training [time: 67.56s, train loss: 3058.2447]
Thu 08 Sep 2022 11:42:22 INFO  epoch 31 training [time: 67.65s, train loss: 3051.8103]
Thu 08 Sep 2022 11:42:23 INFO  epoch 31 evaluating [time: 0.21s, valid_score: 0.233300]
Thu 08 Sep 2022 11:42:23 INFO  valid result: 
recall@3 : 0.0498    recall@5 : 0.0795    recall@10 : 0.1294    recall@20 : 0.2333    ndcg@3 : 0.0362    ndcg@5 : 0.0485    ndcg@10 : 0.0643    ndcg@20 : 0.0903    
Thu 08 Sep 2022 11:42:23 INFO  Saving current best: saved\SHAN2-Sep-08-2022_11-06-04.pth
Thu 08 Sep 2022 11:43:31 INFO  epoch 32 training [time: 67.67s, train loss: 3044.2956]
Thu 08 Sep 2022 11:44:38 INFO  epoch 33 training [time: 67.63s, train loss: 3037.3378]
Thu 08 Sep 2022 11:44:39 INFO  epoch 33 evaluating [time: 0.21s, valid_score: 0.231200]
Thu 08 Sep 2022 11:44:39 INFO  valid result: 
recall@3 : 0.0414    recall@5 : 0.0689    recall@10 : 0.1294    recall@20 : 0.2312    ndcg@3 : 0.0307    ndcg@5 : 0.042    ndcg@10 : 0.0617    ndcg@20 : 0.0871    
Thu 08 Sep 2022 11:45:46 INFO  epoch 34 training [time: 67.75s, train loss: 3031.2803]
Thu 08 Sep 2022 11:46:54 INFO  epoch 35 training [time: 67.74s, train loss: 3025.0655]
Thu 08 Sep 2022 11:46:54 INFO  epoch 35 evaluating [time: 0.21s, valid_score: 0.222700]
Thu 08 Sep 2022 11:46:54 INFO  valid result: 
recall@3 : 0.0488    recall@5 : 0.0785    recall@10 : 0.1326    recall@20 : 0.2227    ndcg@3 : 0.034    ndcg@5 : 0.0459    ndcg@10 : 0.0633    ndcg@20 : 0.0861    
Thu 08 Sep 2022 11:48:02 INFO  epoch 36 training [time: 67.64s, train loss: 3016.0852]
Thu 08 Sep 2022 11:49:10 INFO  epoch 37 training [time: 67.88s, train loss: 3011.2114]
Thu 08 Sep 2022 11:49:10 INFO  epoch 37 evaluating [time: 0.21s, valid_score: 0.220600]
Thu 08 Sep 2022 11:49:10 INFO  valid result: 
recall@3 : 0.0488    recall@5 : 0.0732    recall@10 : 0.1294    recall@20 : 0.2206    ndcg@3 : 0.0355    ndcg@5 : 0.0453    ndcg@10 : 0.0631    ndcg@20 : 0.0859    
Thu 08 Sep 2022 11:50:18 INFO  epoch 38 training [time: 67.79s, train loss: 3003.3019]
Thu 08 Sep 2022 11:51:25 INFO  epoch 39 training [time: 67.66s, train loss: 2995.9317]
Thu 08 Sep 2022 11:51:26 INFO  epoch 39 evaluating [time: 0.21s, valid_score: 0.223800]
Thu 08 Sep 2022 11:51:26 INFO  valid result: 
recall@3 : 0.0488    recall@5 : 0.0795    recall@10 : 0.1421    recall@20 : 0.2238    ndcg@3 : 0.0354    ndcg@5 : 0.0479    ndcg@10 : 0.0678    ndcg@20 : 0.0886    
Thu 08 Sep 2022 11:52:34 INFO  epoch 40 training [time: 68.80s, train loss: 2989.9009]
Thu 08 Sep 2022 11:53:43 INFO  epoch 41 training [time: 68.41s, train loss: 2983.0583]
Thu 08 Sep 2022 11:53:43 INFO  epoch 41 evaluating [time: 0.21s, valid_score: 0.228000]
Thu 08 Sep 2022 11:53:43 INFO  valid result: 
recall@3 : 0.0445    recall@5 : 0.0806    recall@10 : 0.141    recall@20 : 0.228    ndcg@3 : 0.031    ndcg@5 : 0.0457    ndcg@10 : 0.0649    ndcg@20 : 0.0869    
Thu 08 Sep 2022 11:54:53 INFO  epoch 42 training [time: 69.51s, train loss: 2977.9716]
Thu 08 Sep 2022 11:56:01 INFO  epoch 43 training [time: 68.90s, train loss: 2969.1470]
Thu 08 Sep 2022 11:56:02 INFO  epoch 43 evaluating [time: 0.21s, valid_score: 0.233300]
Thu 08 Sep 2022 11:56:02 INFO  valid result: 
recall@3 : 0.0467    recall@5 : 0.0785    recall@10 : 0.1326    recall@20 : 0.2333    ndcg@3 : 0.0338    ndcg@5 : 0.0469    ndcg@10 : 0.0644    ndcg@20 : 0.0896    
Thu 08 Sep 2022 11:57:10 INFO  epoch 44 training [time: 68.01s, train loss: 2962.8372]
Thu 08 Sep 2022 11:58:19 INFO  epoch 45 training [time: 68.90s, train loss: 2956.3139]
Thu 08 Sep 2022 11:58:19 INFO  epoch 45 evaluating [time: 0.21s, valid_score: 0.222700]
Thu 08 Sep 2022 11:58:19 INFO  valid result: 
recall@3 : 0.0467    recall@5 : 0.0764    recall@10 : 0.1379    recall@20 : 0.2227    ndcg@3 : 0.0344    ndcg@5 : 0.0465    ndcg@10 : 0.0661    ndcg@20 : 0.0875    
Thu 08 Sep 2022 11:59:28 INFO  epoch 46 training [time: 69.12s, train loss: 2950.8525]
Thu 08 Sep 2022 12:00:37 INFO  epoch 47 training [time: 69.56s, train loss: 2945.8518]
Thu 08 Sep 2022 12:00:38 INFO  epoch 47 evaluating [time: 0.21s, valid_score: 0.223800]
Thu 08 Sep 2022 12:00:38 INFO  valid result: 
recall@3 : 0.0509    recall@5 : 0.0806    recall@10 : 0.1326    recall@20 : 0.2238    ndcg@3 : 0.0363    ndcg@5 : 0.0485    ndcg@10 : 0.0654    ndcg@20 : 0.0882    
Thu 08 Sep 2022 12:01:47 INFO  epoch 48 training [time: 69.38s, train loss: 2937.1167]
Thu 08 Sep 2022 12:02:57 INFO  epoch 49 training [time: 69.70s, train loss: 2932.0490]
Thu 08 Sep 2022 12:02:57 INFO  epoch 49 evaluating [time: 0.21s, valid_score: 0.218500]
Thu 08 Sep 2022 12:02:57 INFO  valid result: 
recall@3 : 0.0488    recall@5 : 0.0742    recall@10 : 0.1432    recall@20 : 0.2185    ndcg@3 : 0.035    ndcg@5 : 0.0455    ndcg@10 : 0.0672    ndcg@20 : 0.0862    
Thu 08 Sep 2022 12:02:57 INFO  Loading model structure and parameters from saved\SHAN2-Sep-08-2022_11-06-04.pth
Thu 08 Sep 2022 12:02:57 INFO  best valid : {'recall@3': 0.0498, 'recall@5': 0.0795, 'recall@10': 0.1294, 'recall@20': 0.2333, 'ndcg@3': 0.0362, 'ndcg@5': 0.0485, 'ndcg@10': 0.0643, 'ndcg@20': 0.0903}
Thu 08 Sep 2022 12:02:57 INFO  test result: {'recall@3': 0.0392, 'recall@5': 0.0679, 'recall@10': 0.122, 'recall@20': 0.1941, 'ndcg@3': 0.0292, 'ndcg@5': 0.041, 'ndcg@10': 0.0585, 'ndcg@20': 0.0768}
